{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyek Data Science Lanjut: Sentiment Analysis\n",
    "- **Nama:** Daffa Suada\n",
    "- **Email:** suadaadaffa@gmail.com\n",
    "- **ID Dicoding:** daffa_suada_i9ug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   reviewId              25000 non-null  object \n",
      " 1   userName              25000 non-null  object \n",
      " 2   userImage             25000 non-null  object \n",
      " 3   content               25000 non-null  object \n",
      " 4   score                 25000 non-null  int64  \n",
      " 5   thumbsUpCount         25000 non-null  int64  \n",
      " 6   reviewCreatedVersion  21478 non-null  object \n",
      " 7   at                    25000 non-null  object \n",
      " 8   replyContent          0 non-null      float64\n",
      " 9   repliedAt             0 non-null      float64\n",
      " 10  appVersion            21478 non-null  object \n",
      "dtypes: float64(2), int64(2), object(7)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset\n",
    "sentiment_df = pd.read_csv('ulasan_coc_english.csv')\n",
    "\n",
    "sentiment_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hitung duplikat\n",
    "sentiment_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewId                    0\n",
       "userName                    0\n",
       "userImage                   0\n",
       "content                     0\n",
       "score                       0\n",
       "thumbsUpCount               0\n",
       "reviewCreatedVersion     3522\n",
       "at                          0\n",
       "replyContent            25000\n",
       "repliedAt               25000\n",
       "appVersion               3522\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hitung kosong\n",
    "sentiment_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Missing Value\n",
    "sentiment_df['reviewCreatedVersion'] = sentiment_df['reviewCreatedVersion'].fillna('0')\n",
    "sentiment_df['replyContent'] = sentiment_df['replyContent'].fillna('0')\n",
    "sentiment_df['repliedAt'] = sentiment_df['repliedAt'].fillna('0')\n",
    "sentiment_df['appVersion'] = sentiment_df['appVersion'].fillna('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewId                0\n",
       "userName                0\n",
       "userImage               0\n",
       "content                 0\n",
       "score                   0\n",
       "thumbsUpCount           0\n",
       "reviewCreatedVersion    0\n",
       "at                      0\n",
       "replyContent            0\n",
       "repliedAt               0\n",
       "appVersion              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59725dd0-533b-47a5-874d-110733215800</td>\n",
       "      <td>This last year has brought some amazing change...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c6c50aae-4e27-4ce3-a8b6-672f812a359a</td>\n",
       "      <td>I've been playing this game for well over 10 y...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0f198656-b900-4b4e-a3ec-383eb3abfa9e</td>\n",
       "      <td>For november 25th version. There are several b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId  \\\n",
       "0  59725dd0-533b-47a5-874d-110733215800   \n",
       "1  c6c50aae-4e27-4ce3-a8b6-672f812a359a   \n",
       "2  0f198656-b900-4b4e-a3ec-383eb3abfa9e   \n",
       "\n",
       "                                             content  score  \n",
       "0  This last year has brought some amazing change...      5  \n",
       "1  I've been playing this game for well over 10 y...      3  \n",
       "2  For november 25th version. There are several b...      1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_col = ['userName', 'userImage', 'reviewCreatedVersion', 'thumbsUpCount', 'reviewCreatedVersion','at', 'replyContent', 'repliedAt', 'appVersion']\n",
    "\n",
    "sentiment_df.drop(columns=drop_col, inplace=True)\n",
    "\n",
    "sentiment_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membersihkan Text\n",
    "\n",
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # menghapus RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # menghapus link\n",
    "    text = re.sub(r'[0-9]+', '', text) # menghapus angka\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # menghapus karakter selain huruf dan angka\n",
    "\n",
    "    text = text.replace('\\n', ' ') # mengganti baris baru dengan spasi\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # menghapus semua tanda baca\n",
    "    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks\n",
    "    return text\n",
    "\n",
    "def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "def filteringText(text):\n",
    "    listStopwords = set(stopwords.words('english'))\n",
    "    listStopwords.update([\"i\", \"iam\",\"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\",\n",
    "    \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\",\n",
    "    \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\",\n",
    "    \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\",\n",
    "    \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\",\n",
    "    \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\",\n",
    "    \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\",\n",
    "    \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
    "    \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\"])\n",
    "    # custom_stopwords = [\n",
    "    # \"game\", \"play\", \"playing\", \"supercell\", \"clash\", \"games\", \"clan\", \"troops\", \"attack\", \"builder\", \"hall\", \"town\", \"level\", \"base\", \"th\", \"years\", \"long\", \"time\", 'get', 'one', 'new', 'please', 'would', 'also', 'even', 'im', 'still', 'make', 'since', 'thing', 'take', 'way', 'want', 'things', 'times', 'every', 'people', 'give', 'days',\n",
    "    # 'update', 'updates', 'upgrade', 'upgrading', 'upgrades', 'account', \n",
    "    #                 'building', 'buildings', 'village', 'townhall', 'bases', 'walls', \n",
    "    #                 'trophies', 'stars', 'star', 'gems', 'gold', 'resources', 'cost', \n",
    "    #                 'chat', 'screen', 'system', 'option', 'super', 'level', 'levels', \n",
    "    #                 'phone', 'app', 'mobile', 'thing', 'things', 'something', 'everything', \n",
    "    #                 'nothing', 'stuff', 'see', 'know', 'think', 'say', 'id', 'us', 'theres', \n",
    "    #                 'though', 'doesnt', 'thats', 'used', 'another', 'making', 'different', \n",
    "    #                 'sometimes', 'first', 'last', 'higher', 'almost', 'going', 'try', \n",
    "    #                 'getting', 'started', 'wait', 'experience', 'gameplay', 'progress', \n",
    "    #                 'hard', 'find', 'attacks', 'attacking', 'keep', 'lose', 'win', 'work', \n",
    "    #                 'support', 'thank', 'thanks', 'hope', 'free', 'buy', 'money', 'spend', \"like\", \"really\", \"dont\", \"much\", \"back\", \"many\", \"lot\", \"always\", \"ever\", \"could\", \"got\", \"use\", \"u\", \"well\", \n",
    "    # \"nice\", \"able\", \"without\", \"start\", \"made\", \"old\", \"overall\", \"max\", \"bit\", \"come\", \"hours\", \"easy\", \"keeps\", \n",
    "    # \"less\", \"anything\", \"guys\", \"two\", \"high\", \"feel\", \"put\", \"reason\", \"however\", \"waiting\", \"pretty\", \"done\", \n",
    "    # \"trying\", \"needs\", \"next\", \"due\", \"enough\", \"someone\", \"months\", \"request\", \"especially\", \"real\", \"already\", \n",
    "    # \"soon\", \"using\", \"far\", \"wish\", \"amount\", \"working\", \"came\", \"right\", \"maybe\", \"look\", \"ago\", \"point\", \"longer\", \n",
    "    # \"anymore\", \"reduce\", \"end\", \"review\", \"says\"]\n",
    "    # listStopwords.update(custom_stopwords)\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered\n",
    "    return text\n",
    "\n",
    "def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata\n",
    "    # Membuat objek stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    " \n",
    "    # Memecah teks menjadi daftar kata\n",
    "    words = text\n",
    " \n",
    "    # Menerapkan stemming pada setiap kata dalam daftar\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    " \n",
    "    # Menggabungkan kata-kata yang telah distem\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    " \n",
    "    return stemmed_text\n",
    "\n",
    "def toSentence(list_words): # Mengubah daftar kata menjadi kalimat\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence\n",
    "\n",
    "slangwords = {\"coc\": \"clash of clans\",\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"ive\": \"i have\",\n",
    "    \"im\": \"i am\",\n",
    "    \"id\": \"i would\",\n",
    "    \"thats\": \"that is\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"wanna\": \"want to\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"aint\": \"is not\",\n",
    "    \"gimme\": \"give me\",\n",
    "    \"lemme\": \"let me\",\n",
    "    \"ya\": \"you\",\n",
    "    \"nah\": \"no\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"tho\": \"though\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"ez\": \"easy\",\n",
    "    \"op\": \"overpowered\",\n",
    "    \"nerf\": \"reduce power\",\n",
    "    \"buff\": \"increase power\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"ty\": \"thank you\"}\n",
    "\n",
    "def fix_slangword(text):\n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() in slangwords :\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else :\n",
    "            fixed_words.append(word)\n",
    "\n",
    "    fixed_text = ' '.join(fixed_words)\n",
    "    return fixed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_casefoldingText</th>\n",
       "      <th>text_slangwords</th>\n",
       "      <th>text_tokenizingText</th>\n",
       "      <th>text_stopword</th>\n",
       "      <th>text_akhir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59725dd0-533b-47a5-874d-110733215800</td>\n",
       "      <td>This last year has brought some amazing change...</td>\n",
       "      <td>5</td>\n",
       "      <td>This last year has brought some amazing change...</td>\n",
       "      <td>this last year has brought some amazing change...</td>\n",
       "      <td>this last year has brought some amazing change...</td>\n",
       "      <td>[this, last, year, has, brought, some, amazing...</td>\n",
       "      <td>[last, year, brought, amazing, changes, classi...</td>\n",
       "      <td>last year brought amazing changes classic game...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c6c50aae-4e27-4ce3-a8b6-672f812a359a</td>\n",
       "      <td>I've been playing this game for well over 10 y...</td>\n",
       "      <td>3</td>\n",
       "      <td>Ive been playing this game for well over  year...</td>\n",
       "      <td>ive been playing this game for well over  year...</td>\n",
       "      <td>i have been playing this game for well over ye...</td>\n",
       "      <td>[i, have, been, playing, this, game, for, well...</td>\n",
       "      <td>[playing, game, well, years, great, latest, up...</td>\n",
       "      <td>playing game well years great latest update ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0f198656-b900-4b4e-a3ec-383eb3abfa9e</td>\n",
       "      <td>For november 25th version. There are several b...</td>\n",
       "      <td>1</td>\n",
       "      <td>For november th version There are several bugs...</td>\n",
       "      <td>for november th version there are several bugs...</td>\n",
       "      <td>for november th version there are several bugs...</td>\n",
       "      <td>[for, november, th, version, there, are, sever...</td>\n",
       "      <td>[november, th, version, several, bugs, regardi...</td>\n",
       "      <td>november th version several bugs regarding cla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId  \\\n",
       "0  59725dd0-533b-47a5-874d-110733215800   \n",
       "1  c6c50aae-4e27-4ce3-a8b6-672f812a359a   \n",
       "2  0f198656-b900-4b4e-a3ec-383eb3abfa9e   \n",
       "\n",
       "                                             content  score  \\\n",
       "0  This last year has brought some amazing change...      5   \n",
       "1  I've been playing this game for well over 10 y...      3   \n",
       "2  For november 25th version. There are several b...      1   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  This last year has brought some amazing change...   \n",
       "1  Ive been playing this game for well over  year...   \n",
       "2  For november th version There are several bugs...   \n",
       "\n",
       "                                text_casefoldingText  \\\n",
       "0  this last year has brought some amazing change...   \n",
       "1  ive been playing this game for well over  year...   \n",
       "2  for november th version there are several bugs...   \n",
       "\n",
       "                                     text_slangwords  \\\n",
       "0  this last year has brought some amazing change...   \n",
       "1  i have been playing this game for well over ye...   \n",
       "2  for november th version there are several bugs...   \n",
       "\n",
       "                                 text_tokenizingText  \\\n",
       "0  [this, last, year, has, brought, some, amazing...   \n",
       "1  [i, have, been, playing, this, game, for, well...   \n",
       "2  [for, november, th, version, there, are, sever...   \n",
       "\n",
       "                                       text_stopword  \\\n",
       "0  [last, year, brought, amazing, changes, classi...   \n",
       "1  [playing, game, well, years, great, latest, up...   \n",
       "2  [november, th, version, several, bugs, regardi...   \n",
       "\n",
       "                                          text_akhir  \n",
       "0  last year brought amazing changes classic game...  \n",
       "1  playing game well years great latest update ho...  \n",
       "2  november th version several bugs regarding cla...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Membersihkan text dan menyimpannya ke `text_clean`\n",
    "sentiment_df['text_clean'] = sentiment_df['content'].apply(cleaningText)\n",
    "\n",
    "# Mengubah huruf dalam text menjadi huruf kecil\n",
    "sentiment_df['text_casefoldingText'] = sentiment_df['text_clean'].apply(casefoldingText)\n",
    "\n",
    "# Mengganti kata kata slang dengan kata kata standar\n",
    "sentiment_df['text_slangwords'] = sentiment_df['text_casefoldingText'].apply(fix_slangword)\n",
    "\n",
    "# Memecah teks menjadi token (kata-kata)\n",
    "sentiment_df['text_tokenizingText'] = sentiment_df['text_slangwords'].apply(tokenizingText)\n",
    "\n",
    "# Menghapus kata-kata stop atau kata kata umum\n",
    "sentiment_df['text_stopword'] = sentiment_df['text_tokenizingText'].apply(filteringText)\n",
    "\n",
    "# Menggabungkan token-token menjadi kalimat\n",
    "sentiment_df['text_akhir'] = sentiment_df['text_stopword'].apply(toSentence)\n",
    "\n",
    "\n",
    "sentiment_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/daffasuada/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('game', 39812), ('time', 10795), ('like', 7705), ('play', 6855), ('playing', 6563), ('good', 6417), ('get', 6338), ('upgrade', 5766), ('update', 5337), ('one', 5220), ('new', 5189), ('base', 5187), ('would', 5125), ('years', 4887), ('fun', 4464), ('clash', 4398), ('really', 4365), ('great', 4170), ('attack', 4154), ('clans', 4144), ('supercell', 4098), ('please', 4080), ('troops', 3975), ('also', 3813), ('much', 3788), ('love', 3786), ('even', 3780), ('clan', 3713), ('still', 3601), ('make', 3511), ('th', 3425), ('fix', 3385), ('problem', 3376), ('back', 3062), ('long', 3031), ('games', 2998), ('level', 2899), ('best', 2868), ('builder', 2858), ('since', 2803), ('hall', 2691), ('account', 2624), ('many', 2603), ('town', 2597), ('takes', 2577), ('players', 2547), ('thing', 2513), ('want', 2418), ('lot', 2391), ('way', 2344), ('played', 2308), ('take', 2265), ('need', 2243), ('money', 2209), ('things', 2169), ('times', 2073), ('always', 2042), ('every', 2037), ('people', 2035), ('war', 1996), ('give', 1969), ('upgrades', 1927), ('days', 1894), ('issue', 1883), ('upgrading', 1875), ('ever', 1874), ('stars', 1859), ('think', 1852), ('building', 1847), ('lost', 1807), ('keep', 1792), ('updates', 1736), ('could', 1728), ('something', 1700), ('star', 1685), ('strategy', 1653), ('got', 1651), ('better', 1650), ('village', 1607), ('gold', 1582), ('use', 1572), ('app', 1549), ('trophies', 1547), ('help', 1527), ('everything', 1510), ('player', 1486), ('getting', 1473), ('connection', 1473), ('gems', 1458), ('never', 1454), ('add', 1449), ('go', 1446), ('try', 1416), ('amazing', 1408), ('heroes', 1395), ('well', 1366), ('higher', 1347), ('know', 1342), ('battle', 1338), ('pay', 1335), ('pass', 1316), ('nice', 1310), ('see', 1292), ('phone', 1288), ('day', 1275), ('able', 1272), ('wait', 1264), ('awesome', 1261), ('work', 1253), ('without', 1248), ('last', 1245), ('first', 1227), ('attacking', 1226), ('hope', 1222), ('though', 1201), ('makes', 1194), ('hero', 1174), ('going', 1171), ('graphics', 1166), ('screen', 1165), ('win', 1163), ('support', 1150), ('progress', 1143), ('start', 1143), ('made', 1141), ('chat', 1138), ('used', 1110), ('free', 1109), ('tried', 1100), ('spend', 1098), ('buildings', 1095), ('say', 1093), ('old', 1090), ('change', 1089), ('overall', 1087), ('attacks', 1082), ('us', 1082), ('hard', 1080), ('loot', 1079), ('sometimes', 1074), ('find', 1071), ('friends', 1070), ('thank', 1054), ('experience', 1045), ('open', 1032), ('max', 1027), ('gets', 991), ('build', 988), ('bug', 974), ('theres', 972)]\n"
     ]
    }
   ],
   "source": [
    "# Cari kata kata yang sering muncul. \n",
    "\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sentiment = sentiment_df['text_akhir']\n",
    "text = \" \".join(text_sentiment).lower()\n",
    "words = nltk.word_tokenize(text.lower())\n",
    "word_freq = Counter(words)\n",
    "print(word_freq.most_common(150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laveling Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_type_Vanbes\n",
      "POSITIVE    17378\n",
      "NEGATIVE     7349\n",
      "NEUTRAL       273\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_type_Vanbes</th>\n",
       "      <th>compound</th>\n",
       "      <th>text_akhir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.9678</td>\n",
       "      <td>last year brought amazing changes classic game...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.7850</td>\n",
       "      <td>playing game well years great latest update ho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment_type_Vanbes  compound  \\\n",
       "0              POSITIVE    0.9678   \n",
       "1              POSITIVE    0.7850   \n",
       "\n",
       "                                          text_akhir  \n",
       "0  last year brought amazing changes classic game...  \n",
       "1  playing game well years great latest update ho...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labeling Data Using Vader\n",
    "sentiment_vader_df = sentiment_df.copy()\n",
    "\n",
    "labeling = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Hitung skor sentimen dengan VADER\n",
    "sentiment_vader_df['scores'] = sentiment_vader_df['content'].apply(\n",
    "    lambda text : labeling.polarity_scores(text)\n",
    ")\n",
    "\n",
    "# Ambil nilai compound score\n",
    "sentiment_vader_df['compound'] = sentiment_vader_df['scores'].apply(lambda score: score['compound'])\n",
    "\n",
    "# Kategorisasi sentimen sesuai aturan VADER\n",
    "sentiment_vader_df['sentiment_type_Vanbes'] = sentiment_vader_df['compound'].apply(\n",
    "    lambda x: 'POSITIVE' if x > 0 else 'NEGATIVE' if x < -0 else 'NEUTRAL'\n",
    ")\n",
    "\n",
    "# Hitung jumlah masing-masing sentimen\n",
    "print(sentiment_vader_df['sentiment_type_Vanbes'].value_counts())\n",
    "sentiment_vader_df[['sentiment_type_Vanbes','compound', 'text_akhir']].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          text_akhir  polarity     label\n",
      "0  last year brought amazing changes classic game...  0.156239  Positife\n",
      "1  playing game well years great latest update ho...  0.128571  Positife\n",
      "2  november th version several bugs regarding cla...  0.075000  Positife\n",
      "3  still getting disconnected every time open app... -0.268519  Negative\n",
      "4  play years catch someone downloaded game month... -0.255556  Negative\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "Positife    13586\n",
       "Negative    10915\n",
       "Neutral       499\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labeling Data Using TextBlob\n",
    "sentiment_textblob_df = sentiment_df.copy()\n",
    "sentiment_textblob_df['polarity'] = sentiment_textblob_df['text_akhir'].apply(\n",
    "    lambda text : TextBlob(text).sentiment.polarity\n",
    ")\n",
    "\n",
    "sentiment_textblob_df['label'] = sentiment_textblob_df['polarity'].apply(\n",
    "    lambda polarity : 'Positife' if polarity > 0 else 'Negative' if polarity < -0 else 'Neutral'\n",
    ")\n",
    "\n",
    "print(sentiment_textblob_df[['text_akhir', 'polarity','label']].head())\n",
    "sentiment_textblob_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skema Pelatihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ekstraksi Fitur: TF-IDF,    Pembagian Data: 80/20, Labeling: TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pisahkan data menjadi fitur (tweet) dan label (sentimen)\n",
    "X = sentiment_textblob_df['text_akhir']\n",
    "y = sentiment_textblob_df['label']\n",
    "\n",
    "# Ekstraksi Fitur : TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=3500, min_df=8, max_df=0.90)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Bagi data menjadi data latih dan data uji\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine - accuracy_train: 0.98675\n",
      "Support Vector Machine - accuracy_test: 0.8772\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(kernel='rbf',random_state=42, class_weight= 'balanced')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_pred_train = svm_model.predict(X_train)\n",
    "y_pred_test = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluasi model\n",
    "accuracy_train = accuracy_score(y_pred_train, y_train)\n",
    "accuracy_test = accuracy_score(y_pred_test, y_test)\n",
    "\n",
    "# Menampilkan akurasi\n",
    "print('Support Vector Machine - accuracy_train:', accuracy_train)\n",
    "print('Support Vector Machine - accuracy_test:', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - accuracy_train: 0.941\n",
      "Logistic Regression - accuracy_test: 0.9006\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_pred_train = lr_model.predict(X_train)\n",
    "y_pred_test = lr_model.predict(X_test)\n",
    "\n",
    "#Evaluasi model\n",
    "accuracy_train = accuracy_score(y_pred_train, y_train)\n",
    "accuracy_test = accuracy_score(y_pred_test, y_test)\n",
    "\n",
    "# Menampilkan akurasi\n",
    "print('Logistic Regression - accuracy_train:', accuracy_train)\n",
    "print('Logistic Regression - accuracy_test:', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - accuracy_train: 0.7256\n",
      "Random Forest - accuracy_test: 0.6904\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_pred_train = random_forest.predict(X_train)\n",
    "y_pred_test = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluasi model\n",
    "accuracy_train = accuracy_score(y_pred_train, y_train)\n",
    "accuracy_test = accuracy_score(y_pred_test, y_test)\n",
    "\n",
    "# Menampilkan akurasi\n",
    "print('Random Forest - accuracy_train:', accuracy_train)\n",
    "print('Random Forest - accuracy_test:', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ekstraksi Fitur: Word2Vec,    Pembagian Data: 80/20, Labeling: Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi teks\n",
    "sentiment_vader_df[\"tokens\"] = sentiment_vader_df[\"text_akhir\"].apply(word_tokenize)\n",
    "sentences = sentiment_vader_df[\"tokens\"]\n",
    "\n",
    "# Train Word2Vec\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "def sentence_vector(sentence, model):\n",
    "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
    "\n",
    "# Konversi semua teks menjadi vektor\n",
    "X = np.array([sentence_vector(sentence, model) for sentence in sentences])\n",
    "y = sentiment_vader_df[\"sentiment_type_Vanbes\"]\n",
    "\n",
    "# Bagi data menjadi data latih dan data uji\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine - accuracy_train: 0.7842\n",
      "Support Vector Machine - accuracy_test: 0.7536\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(kernel='rbf',random_state=42, class_weight= 'balanced')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_pred_train = svm_model.predict(X_train)\n",
    "y_pred_test = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluasi model\n",
    "accuracy_train = accuracy_score(y_pred_train, y_train)\n",
    "accuracy_test = accuracy_score(y_pred_test, y_test)\n",
    "\n",
    "# Menampilkan akurasi\n",
    "print('Support Vector Machine - accuracy_train:', accuracy_train)\n",
    "print('Support Vector Machine - accuracy_test:', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - accuracy_train: 0.8138\n",
      "Logistic Regression - accuracy_test: 0.7974\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_pred_train = lr_model.predict(X_train)\n",
    "y_pred_test = lr_model.predict(X_test)\n",
    "\n",
    "#Evaluasi model\n",
    "accuracy_train = accuracy_score(y_pred_train, y_train)\n",
    "accuracy_test = accuracy_score(y_pred_test, y_test)\n",
    "\n",
    "# Menampilkan akurasi\n",
    "print('Logistic Regression - accuracy_train:', accuracy_train)\n",
    "print('Logistic Regression - accuracy_test:', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - accuracy_train: 0.88555\n",
      "Random Forest - accuracy_test: 0.7844\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_pred_train = random_forest.predict(X_train)\n",
    "y_pred_test = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluasi model\n",
    "accuracy_train = accuracy_score(y_pred_train, y_train)\n",
    "accuracy_test = accuracy_score(y_pred_test, y_test)\n",
    "\n",
    "# Menampilkan akurasi\n",
    "print('Random Forest - accuracy_train:', accuracy_train)\n",
    "print('Random Forest - accuracy_test:', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ekstraksi Fitur: TF-IDF,    Pembagian Data: 70/30    , Labeling: TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pisahkan data menjadi fitur (tweet) dan label (sentimen)\n",
    "X = sentiment_textblob_df['text_akhir']\n",
    "y = sentiment_textblob_df['label']\n",
    "\n",
    "# Ekstraksi Fitur : TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=3500, min_df=8, max_df=0.90)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Bagi data menjadi data latih dan data uji\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine - accuracy_train: 0.9866285714285714\n",
      "Support Vector Machine - accuracy_test: 0.8705333333333334\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(kernel='rbf',random_state=42, class_weight= 'balanced')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_pred_train = svm_model.predict(X_train)\n",
    "y_pred_test = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluasi model\n",
    "accuracy_train = accuracy_score(y_pred_train, y_train)\n",
    "accuracy_test = accuracy_score(y_pred_test, y_test)\n",
    "\n",
    "# Menampilkan akurasi\n",
    "print('Support Vector Machine - accuracy_train:', accuracy_train)\n",
    "print('Support Vector Machine - accuracy_test:', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - accuracy_train: 0.9397142857142857\n",
      "Logistic Regression - accuracy_test: 0.8945333333333333\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_pred_train = lr_model.predict(X_train)\n",
    "y_pred_test = lr_model.predict(X_test)\n",
    "\n",
    "#Evaluasi model\n",
    "accuracy_train = accuracy_score(y_pred_train, y_train)\n",
    "accuracy_test = accuracy_score(y_pred_test, y_test)\n",
    "\n",
    "# Menampilkan akurasi\n",
    "print('Logistic Regression - accuracy_train:', accuracy_train)\n",
    "print('Logistic Regression - accuracy_test:', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - accuracy_train: 0.7335428571428572\n",
      "Random Forest - accuracy_test: 0.6878666666666666\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_pred_train = random_forest.predict(X_train)\n",
    "y_pred_test = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluasi model\n",
    "accuracy_train = accuracy_score(y_pred_train, y_train)\n",
    "accuracy_test = accuracy_score(y_pred_test, y_test)\n",
    "\n",
    "# Menampilkan akurasi\n",
    "print('Random Forest - accuracy_train:', accuracy_train)\n",
    "print('Random Forest - accuracy_test:', accuracy_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
